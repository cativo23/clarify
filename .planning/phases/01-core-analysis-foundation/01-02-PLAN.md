---
phase: 01-core-analysis-foundation
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - server/utils/openai-client.ts
autonomous: true
requirements:
  - TIER-01
  - TIER-03

must_haves:
  truths:
    - "OpenAI client loads forensic prompt when analysisType is 'forensic'"
    - "Model selection uses gpt-5 for forensic tier"
    - "Token limits use 120k input / 30k output for forensic"
    - "Prompt file resolution handles all three tiers: basic, premium, forensic"
  artifacts:
    - path: "server/utils/openai-client.ts"
      provides: "AI analysis client with 3-tier support"
      exports: ["analyzeContract"]
  key_links:
    - from: "server/utils/openai-client.ts"
      to: "server/prompts/v2/forensic-analysis-prompt.txt"
      via: "prompt file resolution logic"
      pattern: "forensic-analysis-prompt\\.txt"
    - from: "server/utils/openai-client.ts"
      to: "server/utils/config.ts"
      via: "getPromptConfig import"
      pattern: "getPromptConfig"
---

<objective>
Update the OpenAI client to properly support the Forensic tier with correct prompt loading, model selection, and token limits.

Purpose: The current openai-client.ts only handles 'basic' and 'premium' tiers. It needs to be updated to load the forensic prompt and use gpt-5 model with appropriate token limits.

Output: Modified openai-client.ts that correctly routes all three analysis types to their respective prompts and models.
</objective>

<execution_context>
@/home/cativo23/.claude/get-shit-done/workflows/execute-plan.md
@/home/cativo23/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-core-analysis-foundation/01-CONTEXT.md
@.planning/phases/01-core-analysis-foundation/01-RESEARCH.md
@server/utils/openai-client.ts
@server/utils/config.ts
</context>

<tasks>

<task type="auto">
  <name>Update analyzeContract Function for Forensic Tier Support</name>
  <files>server/utils/openai-client.ts</files>
  <action>
    Modify the analyzeContract function in server/utils/openai-client.ts to support the Forensic tier:

    1. **Update function signature** to accept 'forensic' as a valid analysisType:
       - Change: analysisType: "basic" | "premium" = "premium"
       - To: analysisType: "basic" | "premium" | "forensic" = "premium"

    2. **Update prompt file resolution** (currently lines 83-86):
       - Current logic only handles 'basic' vs everything else (defaults to premium)
       - Update to explicitly handle all three tiers:
         - 'basic' → basic-analysis-prompt.txt
         - 'forensic' → forensic-analysis-prompt.txt
         - 'premium' (default) → analysis-prompt.txt

    3. **Verify token limit handling**:
       - The config already defines forensic limits (120k input, 30k output)
       - Ensure the preprocessing buffer is appropriate for Forensic tier
       - Consider adjusting buffer from 2k to 5k for Forensic to maximize context usage

    4. **Verify model validation**:
       - The ALLOWED_MODELS already includes 'gpt-5'
       - No changes needed to model validation

    5. **Add debug logging** for Forensic tier:
       - Log when Forensic tier is selected
       - Log token usage specific to Forensic analysis
  </action>
  <verify>
    - TypeScript compilation passes without errors
    - analyzeContract function accepts 'forensic' as analysisType
    - Prompt file resolution includes forensic case
    - Running: npm run typecheck
  </verify>
  <done>
    analyzeContract function properly routes 'forensic' analysis type to forensic-analysis-prompt.txt with gpt-5 model and appropriate token limits.
  </done>
</task>

<task type="auto">
  <name>Update Preprocessing Buffer for Forensic Tier</name>
  <files>server/utils/openai-client.ts</files>
  <action>
    Adjust the preprocessing token buffer to be tier-aware for optimal context usage:

    1. **Current behavior** (line ~107): Reserves 2k tokens buffer for all tiers
       - const availableContext = limits.input - 2000;

    2. **Update to use tier-specific buffer**:
       - For 'forensic': Use 5k buffer (maximize 120k context window)
       - For 'premium': Keep 2k buffer
       - For 'basic': Keep 2k buffer

    3. **Implementation**:
       ```typescript
       const buffer = analysisType === "forensic" ? 5000 : 2000;
       const availableContext = limits.input - buffer;
       ```

    4. **Add comment explaining the rationale**:
       - Forensic gets larger buffer because it targets 15k-40k output
       - Basic/Premium target smaller outputs (2.5k/10k respectively)
  </action>
  <verify>
    - Preprocessing buffer logic is tier-aware
    - Forensic tier uses 5k buffer
    - Premium/Basic tiers use 2k buffer
    - Code comment explains the rationale
  </verify>
  <done>
    Preprocessing uses tier-appropriate token buffer, maximizing context window for Forensic analysis.
  </done>
</task>

</tasks>

<verification>
- openai-client.ts compiles without TypeScript errors
- analyzeContract accepts 'forensic' as valid analysisType
- Prompt file path resolves to forensic-analysis-prompt.txt for forensic tier
- Token buffer is tier-aware (5k for forensic, 2k for others)
- All three tiers (basic, premium, forensic) are handled in prompt resolution
</verification>

<success_criteria>
- Running `npm run typecheck` passes
- OpenAI client correctly loads forensic prompt when analysisType='forensic'
- Token limits are appropriately configured per tier
</success_criteria>

<output>
After completion, create .planning/phases/01-core-analysis-foundation/01-02-SUMMARY.md
</output>
